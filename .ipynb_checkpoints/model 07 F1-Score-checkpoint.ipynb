{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = pickle.load(open('./sav/df_subset.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_subset['Category']\n",
    "X = df_subset[df_subset.columns[1:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 38), (2000,), (8000, 38), (8000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape, X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import KFold, cross_val_score, RandomizedSearchCV, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, precision_score, recall_score, make_scorer\n",
    "from collections import Counter\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials, space_eval\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    clf = model\n",
    "    clf.fit(X_train, y_train)\n",
    "    print_result(clf, X_train, X_test, y_train, y_test)\n",
    "    return(clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(clf, X_train, X_test, y_train, y_test):\n",
    "    print('Accuracy Test :', f'{accuracy_score(y_test, clf.predict(X_test)):.4f}', \n",
    "          '| F1 Test :', f'{f1_score(y_test, clf.predict(X_test), pos_label=\"Bad\"):.4f}',\n",
    "          '| Precision Test :', f'{precision_score(y_test, clf.predict(X_test), pos_label=\"Bad\"):.4f}', \n",
    "          '| Recall Test :', f'{recall_score(y_test, clf.predict(X_test), pos_label=\"Bad\"):.4f}', \n",
    "          '| H Test :', f'{H_score(y_test, clf.predict(X_test)):.4f}')\n",
    "    \n",
    "    print('Accuracy Test :', f'{accuracy_score(y_train, clf.predict(X_train)):.4f}', \n",
    "          '| F1 Test :', f'{f1_score(y_train, clf.predict(X_train), pos_label=\"Bad\"):.4f}',\n",
    "          '| Precision Test :', f'{precision_score(y_train, clf.predict(X_train), pos_label=\"Bad\"):.4f}', \n",
    "          '| Recall Test :', f'{recall_score(y_train, clf.predict(X_train), pos_label=\"Bad\"):.4f}', \n",
    "          '| H Test :', f'{H_score(y_train, clf.predict(X_train)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_score(X_train, y_train):\n",
    "    acc = accuracy_score(y_train, X_train)\n",
    "    f1 = f1_score(y_train, X_train, pos_label = \"Bad\")\n",
    "    return(2 / ((1/(acc+0.0000001))+(1/(f1+0.0000001))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian(space, X, y, modelo, nevals):\n",
    "    \n",
    "    f1 = make_scorer(f1_score, pos_label = \"Bad\")\n",
    "    \n",
    "    def objective(space):        \n",
    "        global best_score\n",
    "        model = modelo(**space, random_state = 1)   \n",
    "        cv =  StratifiedKFold(n_splits = 5, random_state = 1)\n",
    "        score = -cross_val_score(model, X, y, cv = cv, scoring = f1, verbose = False).mean()\n",
    "        if (score < best_score):\n",
    "            best_score = score\n",
    "        return score\n",
    "\n",
    "    start = time.time()\n",
    "    rstate = np.random.RandomState(1)\n",
    "    best = fmin(objective, space = space,algo = tpe.suggest, max_evals = nevals, trials = Trials(), rstate = rstate)\n",
    "\n",
    "    print(\"Hyperopt search took %.2f seconds\" % ((time.time() - start)))\n",
    "    print(\"Best score: %.4f \" % (-best_score))\n",
    "    print(\"Best space: \", space_eval(params, best))\n",
    "    return(space_eval(params, best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (KDTree Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = KDTree(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_dist, nearest_ind = tree.query(X_test, k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_knn = [Counter([y_train.iloc[k] for k in x]).most_common(1)[0][0] for x in nearest_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test : 0.6710 | F1 Test : 0.6670 | Precision Test : 0.6752 | Recall Test : 0.6590 | H Test : 0.6690\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Test :', f'{accuracy_score(y_test, pred_knn):.4f}', \n",
    "      '| F1 Test :', f'{f1_score(y_test, pred_knn, pos_label=\"Bad\"):.4f}',\n",
    "      '| Precision Test :', f'{precision_score(y_test, pred_knn, pos_label=\"Bad\"):.4f}', \n",
    "      '| Recall Test :', f'{recall_score(y_test, pred_knn, pos_label=\"Bad\"):.4f}', \n",
    "      '| H Test :', f'{H_score(y_test, pred_knn):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|██▎                                                                                                                 | 1/50 [00:03<02:44,  3.35s/trial, best loss: -0.6575071531773826]"
     ]
    }
   ],
   "source": [
    "params = {'learning_rate':     hp.choice('learning_rate',[0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, \n",
    "                                                          0.0075, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.50, 0.75, 1]), \n",
    "          'n_estimators':      hp.choice('n_estimators', range(1,400)),\n",
    "          'max_depth':         hp.choice('max_depth',range(1,20)),\n",
    "          'min_samples_split': hp.choice('min_samples_split',np.linspace(0.01, 1.0, 10, endpoint=True)),\n",
    "          'min_samples_leaf':  hp.choice('min_samples_leaf',np.linspace(0.01, 0.5, 50, endpoint=True)), \n",
    "          'subsample':         hp.choice('subsample',[1]), \n",
    "          'max_features':      hp.choice('max_features',['sqrt'])}\n",
    "\n",
    "best_score = 1\n",
    "gbt_params = bayesian(params, X_train, y_train, GradientBoostingClassifier, 50)\n",
    "pred_gbt = evaluate_model(GradientBoostingClassifier(**gbt_params, random_state=2), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'bootstrap':         hp.choice('bootstrap',[True, False]),\n",
    "          'max_depth':         hp.choice('max_depth', range(1, 20)),\n",
    "          'max_features':      hp.choice('max_features',['auto', 'sqrt']),\n",
    "          'min_samples_leaf':  hp.choice('min_samples_leaf',np.linspace(0.01, 0.5, 50, endpoint=True)), \n",
    "          'min_samples_split': hp.choice('min_samples_split',np.linspace(0.01, 1.0, 100, endpoint=True)), \n",
    "          'n_estimators':      hp.choice('n_estimators',range(1,400))}\n",
    "\n",
    "best_score = 1\n",
    "rf_params = bayesian(params, X_train, y_train, RandomForestClassifier, 50)\n",
    "pred_rf = evaluate_model(RandomForestClassifier(**rf_params, random_state=2), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate':    hp.choice('learning_rate',[0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, \n",
    "                                                         0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75]), \n",
    "          'max_depth':        hp.choice('max_depth',range(1,20)),\n",
    "          'min_child_weight': hp.choice('min_child_weight',np.linspace(0.01, 1.0, 100, endpoint=True)),\n",
    "          'gamma':            hp.choice('gamma',np.linspace(0.01, 1.0, 100, endpoint=True)), \n",
    "          'colsample_bytree': hp.choice('colsample_bytree',np.linspace(0.0, 1, 101, endpoint=True)), \n",
    "          'n_estimators':     hp.choice('n_estimators', range(1,200))}\n",
    "\n",
    "best_score = 1\n",
    "xgb_params = bayesian(params, X_train, y_train, xgb.XGBClassifier, 50)\n",
    "pred_xgb = evaluate_model(xgb.XGBClassifier(**xgb_params, random_state=2), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"C\":   hp.choice('C',[0.0001, 0.00025, 0.0005, 0.001, 0.0025, 0.005, 0.01, 0.025, 0.05, 0.1]),\n",
    "          \"\"\n",
    "          \"tol\": hp.choice('tol',[0.00001, 0.000025, 0.00005, 0.0001, 0.00025, 0.0005, 0.001, 0.0025, 0.005, 0.01, 0.025, \n",
    "                                  0.05, 0.1])}\n",
    "\n",
    "best_score = 1\n",
    "log_params = bayesian(params, X_train, y_train, LogisticRegression, 50)\n",
    "pred_log = evaluate_model(LogisticRegression(**log_params, max_iter = 1000, random_state=2), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"max_depth\":        hp.choice('max_depth', range(1, 50)),\n",
    "          \"max_features\":     hp.choice('max_features', range(1, 50)),\n",
    "          \"min_samples_leaf\": hp.choice('min_samples_leaf', range(1, 200)),\n",
    "          \"criterion\":        hp.choice('criterion', [\"gini\", \"entropy\"])}\n",
    "\n",
    "best_score = 1\n",
    "tree_params = bayesian(params, X_train, y_train, DecisionTreeClassifier, 50)\n",
    "pred_tree = evaluate_model(DecisionTreeClassifier(**tree_params, random_state=2), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"degree\": hp.choice('degree', [2, 3, 4]),\n",
    "          \"kernel\": hp.choice('kernel', ['poly']), \n",
    "          \"C\":      hp.choice('C', [0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, \n",
    "                                    0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75])}\n",
    "best_score = 1\n",
    "svm_params = bayesian(params, X_train, y_train, SVC, 10)\n",
    "pred_svm = evaluate_model(SVC(**svm_params, random_state=2, probability = True), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {'C':      hp.choice('C', [1, 2, 5, 10, 15, 20]), \n",
    "          'gamma':  hp.choice('gamma', [0.0001, 0.001, 0.01, 0.1]),\n",
    "          'kernel': hp.choice('kernel', ['rbf'])}\n",
    "\n",
    "best_score = 1\n",
    "svm_params_2 = bayesian(params, X_train, y_train, SVC, 10)\n",
    "pred_svm_2 = evaluate_model(SVC(**svm_params_2, random_state=2, probability = True), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37]\n",
    "params = {'iterations':        hp.choice('iterations', range(100, 4000, 25)), \n",
    "          'learning_rate':     hp.choice('learning_rate', [0.001, 0.0025, 0.0075, 0.01, 0.025, 0.05, 0.1]),\n",
    "          'l2_leaf_reg':       hp.choice('l2_leaf_reg', range(1, 10)), \n",
    "          'cat_features':      hp.choice('cat_features', [cat_features]), \n",
    "          'verbose':           hp.choice('verbose', [False])}\n",
    "\n",
    "best_score = 1\n",
    "cat_params = bayesian(params, X_train, y_train, CatBoostClassifier, 10)\n",
    "pred_cat = evaluate_model(CatBoostClassifier(**cat_params, random_state=2), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_params = {'C': 0.1, 'degree': 2, 'kernel': 'poly'}\n",
    "pred_svm = evaluate_model(SVC(**svm_params, random_state=2), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(pred_svm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = (tree,\n",
    "          LogisticRegression(**log_params, max_iter = 1000).fit(X_train, y_train),\n",
    "          SVC(**svm_params, probability = True).fit(X_train, y_train),\n",
    "          SVC(**svm_params_2, probability = True).fit(X_train, y_train),\n",
    "          DecisionTreeClassifier(**tree_params).fit(X_train, y_train),\n",
    "          RandomForestClassifier(**rf_params).fit(X_train, y_train),\n",
    "          GradientBoostingClassifier(**gbt_params).fit(X_train, y_train),\n",
    "          xgb.XGBClassifier(**xgb_params).fit(X_train, y_train),\n",
    "          CatBoostClassifier(**cat_params).fit(X_train, y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(models, open('./sav/model_f1.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
