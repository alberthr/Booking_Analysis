{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/df_features.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Predicted Category for final models (2 categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo estasdisticos por hotel para introducir en el modelo, ya que cada hotel se comporta de manera distina\n",
    "diff_hotels = df[['Hotel_Address','Diff']].groupby('Hotel_Address').describe()\n",
    "diff_hotels = diff_hotels.Diff.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, diff_hotels, on='Hotel_Address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Good    0.831599\n",
       "Bad     0.168401\n",
       "Name: Category, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = np.array(['Bad' if i < 7 else 'Good' for i in df.Reviewer_Score])\n",
    "df.loc[:, 'Category'] = category\n",
    "df.Category.value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance Nationalities and / or Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_df(df, Balance_Nationality, Balance_Category, cut):\n",
    "    df_balance_country = df.copy()\n",
    "    if Balance_Nationality:\n",
    "        df_balance_country = None\n",
    "        for i in list(df.Nationality_Recode.value_counts().index):\n",
    "            nationality = df[df.Nationality_Recode == i]\n",
    "            n = cut\n",
    "            if len(nationality) < n:\n",
    "                n = len(nationality)\n",
    "            nationality = nationality.sample(n, random_state=1)\n",
    "            df_balance_country = pd.concat([df_balance_country, nationality])\n",
    "    \n",
    "    df_balance_class = df_balance_country.copy()\n",
    "    if Balance_Category:\n",
    "        df_balance_class = None\n",
    "        minclass = np.min(df_balance_country.Category.value_counts())\n",
    "        classes = list(df_balance_country.Category.value_counts().index)\n",
    "        for i in classes:\n",
    "            selected_class = df_balance_country[df_balance_country.Category == i].sample(minclass, random_state=1)\n",
    "            df_balance_class = pd.concat([df_balance_class, selected_class])\n",
    "    \n",
    "    return(df_balance_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77270, 78)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced_1 = balance_df(df, Balance_Nationality=False, Balance_Category=True, cut=10000)\n",
    "df_balanced_2 = balance_df(df_balanced_1, Balance_Nationality=True, Balance_Category=True, \n",
    "                         cut=int(np.median(df_balanced_1.Nationality_Recode.value_counts())*1.5))\n",
    "df_balanced_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "North America          11829\n",
       "UK & Ireland           11810\n",
       "Western Europe         11809\n",
       "Middle east            11761\n",
       "Eastern Europe          9058\n",
       "Asia & Pacific          7869\n",
       "Oceania                 7555\n",
       "Sub-Saharian Africa     2023\n",
       "South/Latin America     1614\n",
       "China                   1127\n",
       "Arab States              815\n",
       "Name: Nationality_Recode, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced_2.Nationality_Recode.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data to run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 78)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model = df_balanced_2.sample(n=10000, random_state=1)\n",
    "df_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 43559,  79791, 422368, 495313, 449874,  76606, 475792, 471535,\n",
       "             81853, 315028,\n",
       "            ...\n",
       "            416884, 510852, 511234, 253909, 380063, 257904, 384279,  91680,\n",
       "             67987, 510356],\n",
       "           dtype='int64', length=10000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_categorical = ['Review_Month','City','Pet','Purpose','Whom','Room_Recode','Nationality_Recode','Length_Recode','Stars']\n",
    "x_numerical = ['Average_Score', 'Total_Number_of_Reviews_Reviewer_Has_Given', 'Close_Landmarks', 'Dist_Center', \n",
    "               'Dist_Train', 'Dist_Airport','food_Neg_Hotel','staff_Neg_Hotel', 'location_Neg_Hotel', 'value_Neg_Hotel',\n",
    "               'comfort_Neg_Hotel', 'room_Neg_Hotel', 'facilities_Neg_Hotel','cleanliness_Neg_Hotel', \n",
    "               'food_Pos_Hotel', 'staff_Pos_Hotel','location_Pos_Hotel', 'value_Pos_Hotel', 'comfort_Pos_Hotel',\n",
    "               'room_Pos_Hotel', 'facilities_Pos_Hotel', 'cleanliness_Pos_Hotel','count', 'mean', 'std', 'min', '25%', \n",
    "               '50%', '75%', 'max']\n",
    "\n",
    "x_col = x_categorical + x_numerical\n",
    "y_col = 'Category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numerical = df_model[x_numerical]\n",
    "X_numerical_std = X_numerical.apply(lambda x: ((x-np.mean(x)) / np.std(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recodifico las varariables categoricas en features distintas binarias.\n",
    "df_model['Review_Month'] = df_model['Review_Month'].astype(str)\n",
    "X_categorical = pd.get_dummies(df_model[x_categorical], prefix_sep='_', drop_first=False)\n",
    "X_categorical = X_categorical.fillna('Not Available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_numerical_std, X_categorical], axis=1, sort=False)\n",
    "y = df_model[y_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i only select the optimal features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[['Average_Score', 'Total_Number_of_Reviews_Reviewer_Has_Given', 'mean',\n",
    "       'std', 'max', 'Review_Month_1', 'Review_Month_10', 'Review_Month_2',\n",
    "       'Review_Month_3', 'Review_Month_4', 'Review_Month_7', 'Review_Month_8',\n",
    "       'Review_Month_9', 'City_Amsterdam', 'City_London', 'Pet_With a pet',\n",
    "       'Purpose_Leisure trip', 'Whom_Family with older children',\n",
    "       'Whom_Family with young children', 'Whom_Travelers with friends',\n",
    "       'Room_Recode_Deluxe', 'Room_Recode_Other (Standard)',\n",
    "       'Room_Recode_Studio', 'Nationality_Recode_Arab States',\n",
    "       'Nationality_Recode_Asia & Pacific',\n",
    "       'Nationality_Recode_Eastern Europe', 'Nationality_Recode_Middle east',\n",
    "       'Nationality_Recode_North America', 'Nationality_Recode_Oceania',\n",
    "       'Nationality_Recode_UK & Ireland', 'Nationality_Recode_Western Europe',\n",
    "       'Length_Recode_Stayed 2 nights', 'Length_Recode_Stayed 5 nights',\n",
    "       'Length_Recode_Stayed 6 nights', 'Length_Recode_Stayed 7 nights',\n",
    "       'Length_Recode_Stayed 8 nights', 'Stars_Pension',\n",
    "       'Stars_hotel de 3 estrellas']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 38), (2000,), (8000, 38), (8000,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape, X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, KFold, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, precision_score, recall_score, make_scorer\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials, space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    clf = model\n",
    "    clf.fit(X_train, y_train)\n",
    "    print_result(clf, X_train, X_test, y_train, y_test)\n",
    "    return(clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_grid(model, params, X_train, X_test, y_train, y_test, verbose = 1):\n",
    "    f1 = make_scorer(f1_score, pos_label = \"Bad\")\n",
    "    clf = GridSearchCV(estimator = model, param_grid = params, n_jobs = -1, cv = 5, verbose = verbose)    \n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf.best_params_, clf.best_score_)\n",
    "    print_result(clf, X_train, X_test, y_train, y_test)\n",
    "    return(clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(clf, X_train, X_test, y_train, y_test):\n",
    "    print('Accuracy Test :', f'{accuracy_score(clf.predict(X_test), y_test):.4f}', \n",
    "          '| F1 Test :', f'{f1_score(clf.predict(X_test), y_test, pos_label=\"Bad\"):.4f}',\n",
    "          '| Precision Test :', f'{precision_score(clf.predict(X_test), y_test, pos_label=\"Bad\"):.4f}', \n",
    "          '| H Test :', f'{H_score(clf.predict(X_test), y_test):.4f}')\n",
    "    \n",
    "    print('Accuracy Train:', f'{accuracy_score(clf.predict(X_train), y_train):.4f}', \n",
    "          '| F1 Train:', f'{f1_score(clf.predict(X_train), y_train, pos_label=\"Bad\"):.4f}',\n",
    "          '| Precision Train:', f'{precision_score(clf.predict(X_train), y_train, pos_label=\"Bad\"):.4f}', \n",
    "          '| H Train:', f'{H_score(clf.predict(X_train), y_train):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_score(X_train, y_train):\n",
    "    acc = accuracy_score(X_train, y_train)\n",
    "    f1 = f1_score(X_train, y_train, pos_label = \"Bad\")\n",
    "    return(2 / ((1/(acc+0.00001))+(1/(f1+0.00001))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian(space, X, y, modelo, nevals):\n",
    "    \n",
    "    f1 = make_scorer(f1_score, pos_label = \"Bad\")\n",
    "    H = make_scorer(H_score, greater_is_better=True) \n",
    "    \n",
    "    def objective(space):        \n",
    "        global best_score\n",
    "        model = modelo(**space, random_state = 1)   \n",
    "        cv =  StratifiedKFold(n_splits = 5, random_state = 1)\n",
    "        score = -cross_val_score(model, X, y, cv = cv, scoring = H, verbose = False).mean()\n",
    "        if (score < best_score):\n",
    "            best_score = score\n",
    "        return score\n",
    "\n",
    "    start = time.time()\n",
    "    rstate = np.random.RandomState(1)\n",
    "    best = fmin(\n",
    "      objective, \n",
    "      space = space,\n",
    "      algo = tpe.suggest, \n",
    "      max_evals = nevals,\n",
    "      trials = Trials(),\n",
    "      rstate = rstate)\n",
    "\n",
    "    print(\"Hyperopt search took %.2f seconds for 200 candidates\" % ((time.time() - start)))\n",
    "    print(\"Best score: %.4f \" % (-best_score))\n",
    "    print(\"Best space: \", space_eval(params, best))\n",
    "    return(space_eval(params, best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (KDTree Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.neighbors import KDTree\n",
    "tree = KDTree(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_dist, nearest_ind = tree.query(X_test, k = 114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_knn = [Counter([y_train.iloc[k] for k in x]).most_common(1)[0][0] for x in nearest_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test : 0.6715 | F1 Test : 0.6677 | Precision Test : 0.6600 | H Test : 0.6696\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Test :', f'{accuracy_score(pred_knn, y_test):.4f}', \n",
    "      '| F1 Test :', f'{f1_score(pred_knn, y_test, pos_label=\"Bad\"):.4f}',\n",
    "      '| Precision Test :', f'{precision_score(pred_knn, y_test, pos_label=\"Bad\"):.4f}', \n",
    "      '| H Test :', f'{H_score(pred_knn, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:57<00:00,  2.35s/trial, best loss: -0.6627344838217297]\n",
      "Hyperopt search took 117.58 seconds for 200 candidates\n",
      "Best score: 0.6627 \n",
      "Best space:  {'learning_rate': 0.025, 'max_depth': 14, 'max_features': 'sqrt', 'min_samples_leaf': 0.06999999999999999, 'min_samples_split': 0.01, 'n_estimators': 145, 'subsample': 1}\n",
      "Accuracy Test : 0.6685 | F1 Test : 0.6683 | Precision Test : 0.6680 | H Test : 0.6684\n",
      "Accuracy Train: 0.6693 | F1 Train: 0.6730 | Precision Train: 0.6811 | H Train: 0.6711\n"
     ]
    }
   ],
   "source": [
    "params = {'learning_rate':     hp.choice('learning_rate',[0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, \n",
    "                                                          0.0075, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.50, 0.75, 1]), \n",
    "          'n_estimators':      hp.choice('n_estimators', range(1,400)),\n",
    "          'max_depth':         hp.choice('max_depth',range(1,20)),\n",
    "          'min_samples_split': hp.choice('min_samples_split',np.linspace(0.01, 1.0, 10, endpoint=True)),\n",
    "          'min_samples_leaf':  hp.choice('min_samples_leaf',np.linspace(0.01, 0.5, 50, endpoint=True)), \n",
    "          'subsample':         hp.choice('subsample',[1]), \n",
    "          'max_features':      hp.choice('max_features',['sqrt'])}\n",
    "\n",
    "best_score = 1\n",
    "gbt_params = bayesian(params, X_train, y_train, GradientBoostingClassifier, 50)\n",
    "pred_gbt = evaluate_model(GradientBoostingClassifier(**gbt_params), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [02:01<00:00,  2.42s/trial, best loss: -0.6590016134314729]\n",
      "Hyperopt search took 121.27 seconds for 200 candidates\n",
      "Best score: 0.6590 \n",
      "Best space:  {'bootstrap': True, 'max_depth': 4, 'max_features': 'sqrt', 'min_samples_leaf': 0.01, 'min_samples_split': 0.12, 'n_estimators': 145}\n",
      "Accuracy Test : 0.6640 | F1 Test : 0.6722 | Precision Test : 0.6890 | H Test : 0.6681\n",
      "Accuracy Train: 0.6603 | F1 Train: 0.6713 | Precision Train: 0.6941 | H Train: 0.6657\n"
     ]
    }
   ],
   "source": [
    "params = {'bootstrap':         hp.choice('bootstrap',[True, False]),\n",
    "          'max_depth':         hp.choice('max_depth', range(1, 20)),\n",
    "          'max_features':      hp.choice('max_features',['auto', 'sqrt']),\n",
    "          'min_samples_leaf':  hp.choice('min_samples_leaf',np.linspace(0.01, 0.5, 50, endpoint=True)), \n",
    "          'min_samples_split': hp.choice('min_samples_split',np.linspace(0.01, 1.0, 100, endpoint=True)), \n",
    "          'n_estimators':      hp.choice('n_estimators',range(1,400))}\n",
    "\n",
    "best_score = 1\n",
    "rf_params = bayesian(params, X_train, y_train, RandomForestClassifier, 50)\n",
    "pred_rf = evaluate_model(RandomForestClassifier(**rf_params), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [05:38<00:00,  6.76s/trial, best loss: -0.6654064668150198]\n",
      "Hyperopt search took 338.35 seconds for 200 candidates\n",
      "Best score: 0.6654 \n",
      "Best space:  {'colsample_bytree': 0.25, 'gamma': 0.75, 'learning_rate': 0.5, 'max_depth': 1, 'min_child_weight': 0.3, 'n_estimators': 137}\n",
      "Accuracy Test : 0.6690 | F1 Test : 0.6697 | Precision Test : 0.6710 | H Test : 0.6693\n",
      "Accuracy Train: 0.6686 | F1 Train: 0.6717 | Precision Train: 0.6783 | H Train: 0.6702\n"
     ]
    }
   ],
   "source": [
    "params = {'learning_rate':    hp.choice('learning_rate',[0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, \n",
    "                                                         0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75]), \n",
    "          'max_depth':        hp.choice('max_depth',range(1,20)),\n",
    "          'min_child_weight': hp.choice('min_child_weight',np.linspace(0.01, 1.0, 100, endpoint=True)),\n",
    "          'gamma':            hp.choice('gamma',np.linspace(0.01, 1.0, 100, endpoint=True)), \n",
    "          'colsample_bytree': hp.choice('colsample_bytree',np.linspace(0.0, 1, 101, endpoint=True)), \n",
    "          'n_estimators':     hp.choice('n_estimators', range(1,200))}\n",
    "\n",
    "best_score = 1\n",
    "xgb_params = bayesian(params, X_train, y_train, xgb.XGBClassifier, 50)\n",
    "pred_xgb = evaluate_model(xgb.XGBClassifier(**xgb_params), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:17<00:00,  2.88trial/s, best loss: -0.6652393230847832]\n",
      "Hyperopt search took 17.39 seconds for 200 candidates\n",
      "Best score: 0.6652 \n",
      "Best space:  {'C': 0.1, 'tol': 0.1}\n",
      "Accuracy Test : 0.6660 | F1 Test : 0.6599 | Precision Test : 0.6480 | H Test : 0.6629\n",
      "Accuracy Train: 0.6719 | F1 Train: 0.6706 | Precision Train: 0.6683 | H Train: 0.6712\n"
     ]
    }
   ],
   "source": [
    "params = {\"C\":   hp.choice('C',[0.0001, 0.00025, 0.0005, 0.001, 0.0025, 0.005, 0.01, 0.025, 0.05, 0.1]),\n",
    "          \"\"\n",
    "          \"tol\": hp.choice('tol',[0.00001, 0.000025, 0.00005, 0.0001, 0.00025, 0.0005, 0.001, 0.0025, 0.005, 0.01, 0.025, \n",
    "                                  0.05, 0.1])}\n",
    "\n",
    "best_score = 1\n",
    "log_params = bayesian(params, X_train, y_train, LogisticRegression, 50)\n",
    "pred_log = evaluate_model(LogisticRegression(**log_params, max_iter = 1000), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:10<00:00,  4.69trial/s, best loss: -0.6572264037714508]\n",
      "Hyperopt search took 10.70 seconds for 200 candidates\n",
      "Best score: 0.6572 \n",
      "Best space:  {'criterion': 'entropy', 'max_depth': 3, 'max_features': 35, 'min_samples_leaf': 135}\n",
      "Accuracy Test : 0.6505 | F1 Test : 0.6881 | Precision Test : 0.7710 | H Test : 0.6688\n",
      "Accuracy Train: 0.6441 | F1 Train: 0.6831 | Precision Train: 0.7674 | H Train: 0.6630\n"
     ]
    }
   ],
   "source": [
    "params = {\"max_depth\":        hp.choice('max_depth', range(1, 50)),\n",
    "          \"max_features\":     hp.choice('max_features', range(1, 50)),\n",
    "          \"min_samples_leaf\": hp.choice('min_samples_leaf', range(1, 200)),\n",
    "          \"criterion\":        hp.choice('criterion', [\"gini\", \"entropy\"])}\n",
    "\n",
    "best_score = 1\n",
    "tree_params = bayesian(params, X_train, y_train, DecisionTreeClassifier, 50)\n",
    "pred_tree = evaluate_model(DecisionTreeClassifier(**tree_params), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:16<00:00, 13.64s/trial, best loss: -0.6700120050386014]\n",
      "Hyperopt search took 136.45 seconds for 200 candidates\n",
      "Best score: 0.6700 \n",
      "Best space:  {'C': 0.1, 'degree': 2, 'kernel': 'poly'}\n",
      "Accuracy Test : 0.6615 | F1 Test : 0.6850 | Precision Test : 0.7360 | H Test : 0.6730\n",
      "Accuracy Train: 0.6650 | F1 Train: 0.6927 | Precision Train: 0.7554 | H Train: 0.6786\n"
     ]
    }
   ],
   "source": [
    "params = {\"degree\": hp.choice('degree', [2, 3, 4]),\n",
    "          \"kernel\": hp.choice('kernel', ['poly']), \n",
    "          \"C\":      hp.choice('C', [0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075, \n",
    "                                    0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75])}\n",
    "best_score = 1\n",
    "svm_params = bayesian(params, X_train, y_train, SVC, 10)\n",
    "pred_svm = evaluate_model(SVC(**svm_params), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM (rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:58<00:00, 17.87s/trial, best loss: -0.6688779415682038]\n",
      "Hyperopt search took 178.75 seconds for 200 candidates\n",
      "Best score: 0.6689 \n",
      "Best space:  {'C': 15, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "Accuracy Test : 0.6675 | F1 Test : 0.6677 | Precision Test : 0.6680 | H Test : 0.6676\n",
      "Accuracy Train: 0.6711 | F1 Train: 0.6743 | Precision Train: 0.6811 | H Train: 0.6727\n"
     ]
    }
   ],
   "source": [
    "params = {'C':      hp.choice('C', [1, 2, 5, 10, 15, 20]), \n",
    "          'gamma':  hp.choice('gamma', [0.0001, 0.001, 0.01, 0.1]),\n",
    "          'kernel': hp.choice('kernel', ['rbf'])}\n",
    "\n",
    "best_score = 1\n",
    "svm_params_2 = bayesian(params, X_train, y_train, SVC, 10)\n",
    "pred_svm_2 = evaluate_model(SVC(**svm_params_2), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Test : 0.6615 | F1 Test : 0.6850 | Precision Test : 0.7360 | H Test : 0.6730\n",
      "Accuracy Train: 0.6650 | F1 Train: 0.6927 | Precision Train: 0.7554 | H Train: 0.6786\n"
     ]
    }
   ],
   "source": [
    "svm_params =  {'C': 0.1, 'degree': 2, 'kernel': 'poly'}\n",
    "pred_svm = evaluate_model(SVC(**svm_params), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>Bad</th>\n",
       "      <th>Good</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bad</th>\n",
       "      <td>736</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good</th>\n",
       "      <td>264</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category  Bad  Good\n",
       "row_0              \n",
       "Bad       736   413\n",
       "Good      264   587"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(pred_svm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6406"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(recall_score(pred_svm, y_test, pos_label='Bad'),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
